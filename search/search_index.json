{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Unofficial iRODS Documentation Best Practices and System Administrator documentation for iRODS systems by the community. NOT affiliated with RENCI or the iRODS project. Currently deployed using GitHub Pages - https://kript.github.io/unofficial_irods_documentation/ Roadmap It would be good if this can cover a variety of subjects usefull to someone setting up or managing an iRODS installation. Some ideas might be; # - \"Initial deployment\" # - \"Metrics and monitoring\" # - \"Zone Management\" # - \"Misc best practices\" # - Resources # - \"Compound Trees\" # - \"File system resources\" # - \"S3 resources\" Contributors This site would not be possible without the following contributors - thank you! Clemens Messerschmidt John Constable (ahem) leonardo Sietse Snel","title":"Home"},{"location":"#unofficial-irods-documentation","text":"Best Practices and System Administrator documentation for iRODS systems by the community. NOT affiliated with RENCI or the iRODS project. Currently deployed using GitHub Pages - https://kript.github.io/unofficial_irods_documentation/","title":"Unofficial iRODS Documentation"},{"location":"#roadmap","text":"It would be good if this can cover a variety of subjects usefull to someone setting up or managing an iRODS installation. Some ideas might be; # - \"Initial deployment\" # - \"Metrics and monitoring\" # - \"Zone Management\" # - \"Misc best practices\" # - Resources # - \"Compound Trees\" # - \"File system resources\" # - \"S3 resources\"","title":"Roadmap"},{"location":"#contributors","text":"This site would not be possible without the following contributors - thank you! Clemens Messerschmidt John Constable (ahem) leonardo Sietse Snel","title":"Contributors"},{"location":"About/","text":"unofficial_irods_documentation Best Practices and System Administrator documentation for iRODS systems by the community. NOT affiliated with RENCI or the iRODS project. Currently deployed using GitHub Pages - https://kript.github.io/unofficial_irods_documentation/","title":"About"},{"location":"About/#unofficial_irods_documentation","text":"Best Practices and System Administrator documentation for iRODS systems by the community. NOT affiliated with RENCI or the iRODS project. Currently deployed using GitHub Pages - https://kript.github.io/unofficial_irods_documentation/","title":"unofficial_irods_documentation"},{"location":"Containerisation/","text":"Container Options Singularity icommands Contributed by leonardo ; # The simplest singularity def file to get you started. Customise appropriately # cat icommands.def Bootstrap: docker From: centos:centos7 %post yum upgrade -y yum install -y wget epel-release yum install -y python36-jsonschema python36-psutil rpm --import https://packages.irods.org/irods-signing-key.asc wget -qO - https://packages.irods.org/renci-irods.yum.repo | tee /etc/yum.repos.d/renci-irods.yum.repo yum install -y irods-icommands.x86_64 %environment export LC_ALL=C %runscript exec \"$@\" # Build with # sudo singularity build icommands.sif icommands.def An example to deploy to an HPC environment that uses Modules with; help([==[ Description =========== iRODS icommands v4.3.0. More information ================ - Homepage: https://github.com/irods/irods_client_icommands ]==]) conflict(\"icommands\") set_alias(\"iinit\",\"/path/to/icommands.sif iinit\") set_alias(\"ils\",\"/path/to/icommands.sif ils\") # add other icommands in a similar fashion. This can be done automatically via ihelp | awk '$2==\"-\"{print \"set_alias(\\\"\"$1\"\\\",\\\"/path/to/icommands.sif \"$1\"\\\")\"}' >> icommands/4.3.0.lua # Place icommands/4.3.0.lua in an appropriate location on your system Some links fro the above; easybuild EasyBuild is a software build and installation framework that allows you to manage (scientific) software on High Performance Computing (HPC) systems in an efficient way. Lmod ; Lmod is a Lua based module system that easily handles the MODULEPATH Hierarchical problem. Environment Modules provide a convenient way to dynamically change the users\u2019 environment through modulefiles. This includes easily adding or removing directories to the PATH environment variable. Modulefiles for Library packages provide environment variables that specify where the library and header files can be found.","title":"Containerisation"},{"location":"Containerisation/#container-options","text":"","title":"Container Options"},{"location":"Containerisation/#singularity","text":"","title":"Singularity"},{"location":"Containerisation/#icommands","text":"Contributed by leonardo ; # The simplest singularity def file to get you started. Customise appropriately # cat icommands.def Bootstrap: docker From: centos:centos7 %post yum upgrade -y yum install -y wget epel-release yum install -y python36-jsonschema python36-psutil rpm --import https://packages.irods.org/irods-signing-key.asc wget -qO - https://packages.irods.org/renci-irods.yum.repo | tee /etc/yum.repos.d/renci-irods.yum.repo yum install -y irods-icommands.x86_64 %environment export LC_ALL=C %runscript exec \"$@\" # Build with # sudo singularity build icommands.sif icommands.def An example to deploy to an HPC environment that uses Modules with; help([==[ Description =========== iRODS icommands v4.3.0. More information ================ - Homepage: https://github.com/irods/irods_client_icommands ]==]) conflict(\"icommands\") set_alias(\"iinit\",\"/path/to/icommands.sif iinit\") set_alias(\"ils\",\"/path/to/icommands.sif ils\") # add other icommands in a similar fashion. This can be done automatically via ihelp | awk '$2==\"-\"{print \"set_alias(\\\"\"$1\"\\\",\\\"/path/to/icommands.sif \"$1\"\\\")\"}' >> icommands/4.3.0.lua # Place icommands/4.3.0.lua in an appropriate location on your system Some links fro the above; easybuild EasyBuild is a software build and installation framework that allows you to manage (scientific) software on High Performance Computing (HPC) systems in an efficient way. Lmod ; Lmod is a Lua based module system that easily handles the MODULEPATH Hierarchical problem. Environment Modules provide a convenient way to dynamically change the users\u2019 environment through modulefiles. This includes easily adding or removing directories to the PATH environment variable. Modulefiles for Library packages provide environment variables that specify where the library and header files can be found.","title":"icommands"},{"location":"Database_tuning/","text":"Database tuning The iCAT database is a key part of iRODS. In order to get iRODS to perform well, it is important to ensure that you don't have any bottlenecks in your database configuration. You can find generic advice about database tuning in Use the Index, Luke . Some generic high-level advice: * Newer database versions generally have performance improvements. If you have a very old database version, consider upgrading. * If only some queries are very slow, verify that your iCAT database has indexes that the database can use to efficiently handle these queries. Indexes can potentially increase performance of particular queries a lot, although that can be at the expense of disk space and speed of adding new data. * Ensure that your database has adequate resources, such as memory, and check that your database is configured to use these resources. Consider running your iCAT database on a separate server, so that it doesn't have to compete with other services for resources. Finding slow queries One way to improve database performance is to first determine what queries are taking a lot of time, and improve their performance (if they are used frequently or getting them to run fast is important for other reasons). PostgreSQL can be configured to log any slow queries using the log_min_duration_statement setting. For example, in order to log any queries that take more than one second (1000 ms), put the following in your postgresql.conf and reload the configuration ( pg_ctl reload ): log_min_duration_statement = 1000 An example of a logged statement could be: LOG: duration: 119619.149 ms execute <unnamed>: select distinct R_DATA_MAIN.data_name ,R_COLL_MAIN.coll_name ,min(R_DATA_MAIN.create_ts ) ,max(R_DATA_MAIN.modify_ts ) ,R_DATA_MAIN.data_size from R_DATA_MAIN , R_COLL_MAIN where upper (R_COLL_MAIN.coll_name ) LIKE $1 AND upper (R_DATA_MAIN.data_name ) LIKE $2 AND R_COLL_MAIN.coll_id = R_DATA_MAIN.coll_id group by R_DATA_MAIN.data_name ,R_COLL_MAIN.coll_name ,R_DATA_MAIN.data_size order by R_DATA_MAIN.data_name, R_COLL_MAIN.coll_name, R_DATA_MAIN.data_name DETAIL: parameters: $1 = '%COLL%', $2 = '%MYDATAOBJECT%' This is a SELECT query that performs a case-insensitive wildcard search for data objects with a particular collection and data object name. Finding the cause of slow queries Generally speaking, it's advisable to test any queries and potential improvements to make them run faster on a test server. When measuring query time, running on a test server reduces the risk that query time is affected by other operations. You can also test UPDATE, INSERT and DELETE queries without having to worry about tests affecting your production database. If you use PostgreSQL, you can use the iCAT data generator to produce a database with synthetic data for performance testing. If you find a query that takes a lot of time, you can retrieve the query plan and statistics using the EXPLAIN ANALYZE command. Example output on a test server with a smaller database: icat_medium=# explain analyze select distinct R_DATA_MAIN.data_name ,R_COLL_MAIN.coll_name ,min(R_DATA_MAIN.create_ts ) ,max(R_DATA_MAIN.modify_ts ) ,R_DATA_MAIN.data_size from R_DATA_MAIN , R_COLL_MAIN where upper (R_COLL_MAIN.coll_name ) LIKE '%BAR%' AND upper (R_DATA_MAIN.data_name ) LIKE '%FOO%' AND R_COLL_MAIN.coll_id = R_DATA_MAIN.coll_id group by R_DATA_MAIN.data_name ,R_COLL_MAIN.coll_name ,R_DATA_MAIN.data_size order by R_DATA_MAIN.data_name, R_COLL_MAIN.coll_name, R_DATA_MAIN.data_name; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ Unique (cost=122590.55..122710.55 rows=8000 width=190) (actual time=1220.557..1236.456 rows=0 loops=1) -> Sort (cost=122590.55..122610.55 rows=8000 width=190) (actual time=1220.556..1236.455 rows=0 loops=1) Sort Key: r_data_main.data_name, r_coll_main.coll_name, (min((r_data_main.create_ts)::text)), (max((r_data_main.modify_ts)::text)), r_data_main.data_size Sort Method: quicksort Memory: 25kB -> Finalize GroupAggregate (cost=121055.85..122071.92 rows=8000 width=190) (actual time=1220.541..1236.440 rows=0 loops=1) Group Key: r_data_main.data_name, r_coll_main.coll_name, r_data_main.data_size -> Gather Merge (cost=121055.85..121908.59 rows=6666 width=190) (actual time=1220.539..1236.437 rows=0 loops=1) Workers Planned: 2 Workers Launched: 2 -> Partial GroupAggregate (cost=120055.82..120139.15 rows=3333 width=190) (actual time=1202.909..1202.912 rows=0 loops=3) Group Key: r_data_main.data_name, r_coll_main.coll_name, r_data_main.data_size -> Sort (cost=120055.82..120064.16 rows=3333 width=148) (actual time=1202.908..1202.910 rows=0 loops=3) Sort Key: r_data_main.data_name, r_coll_main.coll_name, r_data_main.data_size Sort Method: quicksort Memory: 25kB Worker 0: Sort Method: quicksort Memory: 25kB Worker 1: Sort Method: quicksort Memory: 25kB -> Nested Loop (cost=0.43..119860.80 rows=3333 width=148) (actual time=1202.869..1202.871 rows=0 loops=3) -> Parallel Seq Scan on r_coll_main (cost=0.00..42088.00 rows=16667 width=105) (actual time=1202.868..1202.868 rows=0 loops=3) Filter: (upper((coll_name)::text) ~~ '%BAR%'::text) Rows Removed by Filter: 333333 -> Index Scan using idx_data_main3 on r_data_main (cost=0.43..4.66 rows=1 width=59) (never executed) Index Cond: (coll_id = r_coll_main.coll_id) Filter: (upper((data_name)::text) ~~ '%FOO%'::text) Planning Time: 4.643 ms Execution Time: 1236.571 ms (25 rows) In this example, PostgreSQL has to scan all rows in the collection table ( r_coll_main ), since it doesn't have a suitable index. Indexes Since the query uses a wildcard search, we would need to add an index that supports wildcard searches, like a GIN index . If you don't have the pg_trgm extension installed yet, install it first: icat_medium=# CREATE EXTENSION pg_trgm; CREATE EXTENSION Now add a matching index: icat_medium=# CREATE INDEX idx_coll_gin1 ON r_coll_main USING gin (upper((coll_name)) gin_trgm_ops); CREATE INDEX This speeds up the query by a factor of roughly 100: icat_medium=# explain analyze select distinct R_DATA_MAIN.data_name ,R_COLL_MAIN.coll_name ,min(R_DATA_MAIN.create_ts ) ,max(R_DATA_MAIN.modify_ts ) ,R_DATA_MAIN.data_size from R_DATA_MAIN , R_COLL_MAIN where upper (R_COLL_MAIN.coll_name ) LIKE '%BAR%' AND upper (R_DATA_MAIN.data_name ) LIKE '%FOO%' AND R_COLL_MAIN.coll_id = R_DATA_MAIN.coll_id group by R_DATA_MAIN.data_name ,R_COLL_MAIN.coll_name ,R_DATA_MAIN.data_size order by R_DATA_MAIN.data_name, R_COLL_MAIN.coll_name, R_DATA_MAIN.data_name; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Unique (cost=106751.55..106871.55 rows=8000 width=190) (actual time=7.627..10.721 rows=0 loops=1) -> Sort (cost=106751.55..106771.55 rows=8000 width=190) (actual time=7.626..10.720 rows=0 loops=1) Sort Key: r_data_main.data_name, r_coll_main.coll_name, (min((r_data_main.create_ts)::text)), (max((r_data_main.modify_ts)::text)), r_data_main.data_size Sort Method: quicksort Memory: 25kB -> Finalize GroupAggregate (cost=105216.85..106232.92 rows=8000 width=190) (actual time=7.621..10.714 rows=0 loops=1) Group Key: r_data_main.data_name, r_coll_main.coll_name, r_data_main.data_size -> Gather Merge (cost=105216.85..106069.59 rows=6666 width=190) (actual time=7.620..10.712 rows=0 loops=1) Workers Planned: 2 Workers Launched: 2 -> Partial GroupAggregate (cost=104216.82..104300.15 rows=3333 width=190) (actual time=0.247..0.249 rows=0 loops=3) Group Key: r_data_main.data_name, r_coll_main.coll_name, r_data_main.data_size -> Sort (cost=104216.82..104225.16 rows=3333 width=148) (actual time=0.246..0.247 rows=0 loops=3) Sort Key: r_data_main.data_name, r_coll_main.coll_name, r_data_main.data_size Sort Method: quicksort Memory: 25kB Worker 0: Sort Method: quicksort Memory: 25kB Worker 1: Sort Method: quicksort Memory: 25kB -> Nested Loop (cost=326.43..104021.80 rows=3333 width=148) (actual time=0.208..0.209 rows=0 loops=3) -> Parallel Bitmap Heap Scan on r_coll_main (cost=326.00..26249.00 rows=16667 width=105) (actual time=0.207..0.207 rows=0 loops=3) Recheck Cond: (upper((coll_name)::text) ~~ '%BAR%'::text) -> Bitmap Index Scan on idx_coll_gin1 (cost=0.00..316.00 rows=40000 width=0) (actual time=0.019..0.020 rows=0 loops=1) Index Cond: (upper((coll_name)::text) ~~ '%BAR%'::text) -> Index Scan using idx_data_main3 on r_data_main (cost=0.43..4.66 rows=1 width=59) (never executed) Index Cond: (coll_id = r_coll_main.coll_id) Filter: (upper((data_name)::text) ~~ '%FOO%'::text) Planning Time: 0.458 ms Execution Time: 10.800 ms Beware that there is database overhead to maintain an index. For example, indexes take up disk space. So it's good to check how much disk space is needed on a test server and consider this before adding indexes on production systems. Finally, indexes can increase the time needed for inserting large amounts of data into the database. So if you want to register a large amount of data in the database and be able to search it quickly afterwards, it can make sense to insert the data first and add the indexes later. For example, if you are writing lots of checksums (e.g. you have msiDataObjChksum in an upload rule), you might want to delay adding an index on the checksum values until it actually becomes necessary to get adequate performance for checksum queries. Number of Connections As iRODS does not do connection pooling, every connection to iRODS (including admin tasks) creates a new connection to the database, so you need to be sure that your database is configured for a number of connections relative to the amount of traffic you are expecting (and possibly a healthy overhead to be sure).","title":"Tuning"},{"location":"Database_tuning/#database-tuning","text":"The iCAT database is a key part of iRODS. In order to get iRODS to perform well, it is important to ensure that you don't have any bottlenecks in your database configuration. You can find generic advice about database tuning in Use the Index, Luke . Some generic high-level advice: * Newer database versions generally have performance improvements. If you have a very old database version, consider upgrading. * If only some queries are very slow, verify that your iCAT database has indexes that the database can use to efficiently handle these queries. Indexes can potentially increase performance of particular queries a lot, although that can be at the expense of disk space and speed of adding new data. * Ensure that your database has adequate resources, such as memory, and check that your database is configured to use these resources. Consider running your iCAT database on a separate server, so that it doesn't have to compete with other services for resources.","title":"Database tuning"},{"location":"Database_tuning/#finding-slow-queries","text":"One way to improve database performance is to first determine what queries are taking a lot of time, and improve their performance (if they are used frequently or getting them to run fast is important for other reasons). PostgreSQL can be configured to log any slow queries using the log_min_duration_statement setting. For example, in order to log any queries that take more than one second (1000 ms), put the following in your postgresql.conf and reload the configuration ( pg_ctl reload ): log_min_duration_statement = 1000 An example of a logged statement could be: LOG: duration: 119619.149 ms execute <unnamed>: select distinct R_DATA_MAIN.data_name ,R_COLL_MAIN.coll_name ,min(R_DATA_MAIN.create_ts ) ,max(R_DATA_MAIN.modify_ts ) ,R_DATA_MAIN.data_size from R_DATA_MAIN , R_COLL_MAIN where upper (R_COLL_MAIN.coll_name ) LIKE $1 AND upper (R_DATA_MAIN.data_name ) LIKE $2 AND R_COLL_MAIN.coll_id = R_DATA_MAIN.coll_id group by R_DATA_MAIN.data_name ,R_COLL_MAIN.coll_name ,R_DATA_MAIN.data_size order by R_DATA_MAIN.data_name, R_COLL_MAIN.coll_name, R_DATA_MAIN.data_name DETAIL: parameters: $1 = '%COLL%', $2 = '%MYDATAOBJECT%' This is a SELECT query that performs a case-insensitive wildcard search for data objects with a particular collection and data object name.","title":"Finding slow queries"},{"location":"Database_tuning/#finding-the-cause-of-slow-queries","text":"Generally speaking, it's advisable to test any queries and potential improvements to make them run faster on a test server. When measuring query time, running on a test server reduces the risk that query time is affected by other operations. You can also test UPDATE, INSERT and DELETE queries without having to worry about tests affecting your production database. If you use PostgreSQL, you can use the iCAT data generator to produce a database with synthetic data for performance testing. If you find a query that takes a lot of time, you can retrieve the query plan and statistics using the EXPLAIN ANALYZE command. Example output on a test server with a smaller database: icat_medium=# explain analyze select distinct R_DATA_MAIN.data_name ,R_COLL_MAIN.coll_name ,min(R_DATA_MAIN.create_ts ) ,max(R_DATA_MAIN.modify_ts ) ,R_DATA_MAIN.data_size from R_DATA_MAIN , R_COLL_MAIN where upper (R_COLL_MAIN.coll_name ) LIKE '%BAR%' AND upper (R_DATA_MAIN.data_name ) LIKE '%FOO%' AND R_COLL_MAIN.coll_id = R_DATA_MAIN.coll_id group by R_DATA_MAIN.data_name ,R_COLL_MAIN.coll_name ,R_DATA_MAIN.data_size order by R_DATA_MAIN.data_name, R_COLL_MAIN.coll_name, R_DATA_MAIN.data_name; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ Unique (cost=122590.55..122710.55 rows=8000 width=190) (actual time=1220.557..1236.456 rows=0 loops=1) -> Sort (cost=122590.55..122610.55 rows=8000 width=190) (actual time=1220.556..1236.455 rows=0 loops=1) Sort Key: r_data_main.data_name, r_coll_main.coll_name, (min((r_data_main.create_ts)::text)), (max((r_data_main.modify_ts)::text)), r_data_main.data_size Sort Method: quicksort Memory: 25kB -> Finalize GroupAggregate (cost=121055.85..122071.92 rows=8000 width=190) (actual time=1220.541..1236.440 rows=0 loops=1) Group Key: r_data_main.data_name, r_coll_main.coll_name, r_data_main.data_size -> Gather Merge (cost=121055.85..121908.59 rows=6666 width=190) (actual time=1220.539..1236.437 rows=0 loops=1) Workers Planned: 2 Workers Launched: 2 -> Partial GroupAggregate (cost=120055.82..120139.15 rows=3333 width=190) (actual time=1202.909..1202.912 rows=0 loops=3) Group Key: r_data_main.data_name, r_coll_main.coll_name, r_data_main.data_size -> Sort (cost=120055.82..120064.16 rows=3333 width=148) (actual time=1202.908..1202.910 rows=0 loops=3) Sort Key: r_data_main.data_name, r_coll_main.coll_name, r_data_main.data_size Sort Method: quicksort Memory: 25kB Worker 0: Sort Method: quicksort Memory: 25kB Worker 1: Sort Method: quicksort Memory: 25kB -> Nested Loop (cost=0.43..119860.80 rows=3333 width=148) (actual time=1202.869..1202.871 rows=0 loops=3) -> Parallel Seq Scan on r_coll_main (cost=0.00..42088.00 rows=16667 width=105) (actual time=1202.868..1202.868 rows=0 loops=3) Filter: (upper((coll_name)::text) ~~ '%BAR%'::text) Rows Removed by Filter: 333333 -> Index Scan using idx_data_main3 on r_data_main (cost=0.43..4.66 rows=1 width=59) (never executed) Index Cond: (coll_id = r_coll_main.coll_id) Filter: (upper((data_name)::text) ~~ '%FOO%'::text) Planning Time: 4.643 ms Execution Time: 1236.571 ms (25 rows) In this example, PostgreSQL has to scan all rows in the collection table ( r_coll_main ), since it doesn't have a suitable index.","title":"Finding the cause of slow queries"},{"location":"Database_tuning/#indexes","text":"Since the query uses a wildcard search, we would need to add an index that supports wildcard searches, like a GIN index . If you don't have the pg_trgm extension installed yet, install it first: icat_medium=# CREATE EXTENSION pg_trgm; CREATE EXTENSION Now add a matching index: icat_medium=# CREATE INDEX idx_coll_gin1 ON r_coll_main USING gin (upper((coll_name)) gin_trgm_ops); CREATE INDEX This speeds up the query by a factor of roughly 100: icat_medium=# explain analyze select distinct R_DATA_MAIN.data_name ,R_COLL_MAIN.coll_name ,min(R_DATA_MAIN.create_ts ) ,max(R_DATA_MAIN.modify_ts ) ,R_DATA_MAIN.data_size from R_DATA_MAIN , R_COLL_MAIN where upper (R_COLL_MAIN.coll_name ) LIKE '%BAR%' AND upper (R_DATA_MAIN.data_name ) LIKE '%FOO%' AND R_COLL_MAIN.coll_id = R_DATA_MAIN.coll_id group by R_DATA_MAIN.data_name ,R_COLL_MAIN.coll_name ,R_DATA_MAIN.data_size order by R_DATA_MAIN.data_name, R_COLL_MAIN.coll_name, R_DATA_MAIN.data_name; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Unique (cost=106751.55..106871.55 rows=8000 width=190) (actual time=7.627..10.721 rows=0 loops=1) -> Sort (cost=106751.55..106771.55 rows=8000 width=190) (actual time=7.626..10.720 rows=0 loops=1) Sort Key: r_data_main.data_name, r_coll_main.coll_name, (min((r_data_main.create_ts)::text)), (max((r_data_main.modify_ts)::text)), r_data_main.data_size Sort Method: quicksort Memory: 25kB -> Finalize GroupAggregate (cost=105216.85..106232.92 rows=8000 width=190) (actual time=7.621..10.714 rows=0 loops=1) Group Key: r_data_main.data_name, r_coll_main.coll_name, r_data_main.data_size -> Gather Merge (cost=105216.85..106069.59 rows=6666 width=190) (actual time=7.620..10.712 rows=0 loops=1) Workers Planned: 2 Workers Launched: 2 -> Partial GroupAggregate (cost=104216.82..104300.15 rows=3333 width=190) (actual time=0.247..0.249 rows=0 loops=3) Group Key: r_data_main.data_name, r_coll_main.coll_name, r_data_main.data_size -> Sort (cost=104216.82..104225.16 rows=3333 width=148) (actual time=0.246..0.247 rows=0 loops=3) Sort Key: r_data_main.data_name, r_coll_main.coll_name, r_data_main.data_size Sort Method: quicksort Memory: 25kB Worker 0: Sort Method: quicksort Memory: 25kB Worker 1: Sort Method: quicksort Memory: 25kB -> Nested Loop (cost=326.43..104021.80 rows=3333 width=148) (actual time=0.208..0.209 rows=0 loops=3) -> Parallel Bitmap Heap Scan on r_coll_main (cost=326.00..26249.00 rows=16667 width=105) (actual time=0.207..0.207 rows=0 loops=3) Recheck Cond: (upper((coll_name)::text) ~~ '%BAR%'::text) -> Bitmap Index Scan on idx_coll_gin1 (cost=0.00..316.00 rows=40000 width=0) (actual time=0.019..0.020 rows=0 loops=1) Index Cond: (upper((coll_name)::text) ~~ '%BAR%'::text) -> Index Scan using idx_data_main3 on r_data_main (cost=0.43..4.66 rows=1 width=59) (never executed) Index Cond: (coll_id = r_coll_main.coll_id) Filter: (upper((data_name)::text) ~~ '%FOO%'::text) Planning Time: 0.458 ms Execution Time: 10.800 ms Beware that there is database overhead to maintain an index. For example, indexes take up disk space. So it's good to check how much disk space is needed on a test server and consider this before adding indexes on production systems. Finally, indexes can increase the time needed for inserting large amounts of data into the database. So if you want to register a large amount of data in the database and be able to search it quickly afterwards, it can make sense to insert the data first and add the indexes later. For example, if you are writing lots of checksums (e.g. you have msiDataObjChksum in an upload rule), you might want to delay adding an index on the checksum values until it actually becomes necessary to get adequate performance for checksum queries.","title":"Indexes"},{"location":"Database_tuning/#number-of-connections","text":"As iRODS does not do connection pooling, every connection to iRODS (including admin tasks) creates a new connection to the database, so you need to be sure that your database is configured for a number of connections relative to the amount of traffic you are expecting (and possibly a healthy overhead to be sure).","title":"Number of Connections"},{"location":"Developer/","text":"Making changes to the core project (Copied from 6471 ) If you're trying to make changes to the server, it is going to require additional steps. Here is a quick breakdown of some of the most important directories: lib/api/src: Client-side API functions (basically wrappers around procApiRequest) lib/core/src: Code that is used on the client-side and server-side server/api/src: Server-side API endpoint implementations server/core/src: Server-side utilities, main server startup logic, etc. plugins/database/src: Database specific logic (db_plugin.cpp) General Tips: Functions with an rc prefix represent client-side APIs These calls invoke policy Functions with an rs prefix represent server-side APIs These calls do not invoke policy Functions with a leading underscore are meant for server-side use only The entry point for the server (startup, delay server forking, etc) can be found in rodsServer.cpp The agent factory logic can be found in rodsAgent.cpp When adding a new file to the server, it will require updating one or more CMakeLists.txt files Start with the CMakeLists.txt file located at the root of the repo. That file will help in understanding the structure of the project An easy way to see if your change is being compiled into the server is by adding a log message to rsDataObjPut() and then invoking iput. If everything compiles, then you should see your log message in the log file Last but not least, we highly recommend compiling and installing all of the packages produced by the https://github.com/irods/irods_development_environment. Doing so avoids weird situations around ABI, etc. Custom plugins (Taken from #6458 ) Calling a rule defined in a custom plugin (a .so file) which is defined in the server conf with a particular instance_name . To make this work, your plugin must implement exec_rule_text(). You can use the Logical Quotas REP as a reference for achieving this. See the following: https://github.com/irods/irods_rule_engine_plugin_logical_quotas/blob/df75c52f6d8aa89a2b452aeb56626c2f944b5986/src/main.cpp#L360 https://github.com/irods/irods_rule_engine_plugin_logical_quotas/blob/df75c52f6d8aa89a2b452aeb56626c2f944b5986/src/main.cpp#L331 https://github.com/irods/irods_rule_engine_plugin_logical_quotas/blob/df75c52f6d8aa89a2b452aeb56626c2f944b5986/src/main.cpp#L220 If you want to invoke rules within your plugin from a delay rule, you'll also need to implement exec_rule_expression(). The Logical Quotas REP supports both of these functions and demonstrates how someone can support these through a single function. Keep in mind that the implementation for your plugin could be very different from the Logical Quotas implementation depending on how your plugin accepts input. Logical Quotas accepts JSON as input. If your plugin also accepts JSON, you should be able to use the same pattern as the Logical Quotas REP.","title":"Brief overview"},{"location":"Developer/#making-changes-to-the-core-project","text":"(Copied from 6471 ) If you're trying to make changes to the server, it is going to require additional steps. Here is a quick breakdown of some of the most important directories: lib/api/src: Client-side API functions (basically wrappers around procApiRequest) lib/core/src: Code that is used on the client-side and server-side server/api/src: Server-side API endpoint implementations server/core/src: Server-side utilities, main server startup logic, etc. plugins/database/src: Database specific logic (db_plugin.cpp)","title":"Making changes to the core project"},{"location":"Developer/#general-tips","text":"Functions with an rc prefix represent client-side APIs These calls invoke policy Functions with an rs prefix represent server-side APIs These calls do not invoke policy Functions with a leading underscore are meant for server-side use only The entry point for the server (startup, delay server forking, etc) can be found in rodsServer.cpp The agent factory logic can be found in rodsAgent.cpp When adding a new file to the server, it will require updating one or more CMakeLists.txt files Start with the CMakeLists.txt file located at the root of the repo. That file will help in understanding the structure of the project An easy way to see if your change is being compiled into the server is by adding a log message to rsDataObjPut() and then invoking iput. If everything compiles, then you should see your log message in the log file Last but not least, we highly recommend compiling and installing all of the packages produced by the https://github.com/irods/irods_development_environment. Doing so avoids weird situations around ABI, etc.","title":"General Tips:"},{"location":"Developer/#custom-plugins","text":"(Taken from #6458 ) Calling a rule defined in a custom plugin (a .so file) which is defined in the server conf with a particular instance_name . To make this work, your plugin must implement exec_rule_text(). You can use the Logical Quotas REP as a reference for achieving this. See the following: https://github.com/irods/irods_rule_engine_plugin_logical_quotas/blob/df75c52f6d8aa89a2b452aeb56626c2f944b5986/src/main.cpp#L360 https://github.com/irods/irods_rule_engine_plugin_logical_quotas/blob/df75c52f6d8aa89a2b452aeb56626c2f944b5986/src/main.cpp#L331 https://github.com/irods/irods_rule_engine_plugin_logical_quotas/blob/df75c52f6d8aa89a2b452aeb56626c2f944b5986/src/main.cpp#L220 If you want to invoke rules within your plugin from a delay rule, you'll also need to implement exec_rule_expression(). The Logical Quotas REP supports both of these functions and demonstrates how someone can support these through a single function. Keep in mind that the implementation for your plugin could be very different from the Logical Quotas implementation depending on how your plugin accepts input. Logical Quotas accepts JSON as input. If your plugin also accepts JSON, you should be able to use the same pattern as the Logical Quotas REP.","title":"Custom plugins"},{"location":"PythonAPI/","text":"Python API Some notes on best use, along with some cookbook snippets that people might find helpful. Get the size of all objects under a particular collection Code snippet from #373 total_size_in_bytes = 0 n_data_objs = 0 collection = session.collections.get('/myZone/home/myUser/home/testCollection') for info in collection.walk( ): n_data_objs += len( info[2] ) total_size_in_bytes += sum( d.size for d in info[2] )","title":"Python API"},{"location":"PythonAPI/#python-api","text":"Some notes on best use, along with some cookbook snippets that people might find helpful.","title":"Python API"},{"location":"PythonAPI/#get-the-size-of-all-objects-under-a-particular-collection","text":"Code snippet from #373 total_size_in_bytes = 0 n_data_objs = 0 collection = session.collections.get('/myZone/home/myUser/home/testCollection') for info in collection.walk( ): n_data_objs += len( info[2] ) total_size_in_bytes += sum( d.size for d in info[2] )","title":"Get the size of all objects under a particular collection"},{"location":"Queries/","text":"Querying iRODS Adding specific queries from the command line bash escaping can cause havok when loading in a SQl query, so use this approach to work around that. # create file e.g. # vim lastWeekZeroLengthUnintentional.sql # put that into an environment variable QUERY=$(cat lastWeekZeroLengthUnintentional.sql ) # use the env varaible as a source for the query iadmin asq \"${QUERY}\" lastWeekZeroLengthUnintentional # run the query iquest --no-page --sql lastWeekZeroLengthUnintentional Sending query output by email iquest --no-page --sql lastWeekFilesSingleReplicaNotnoReplRoot | mail -s \"Single Replica Files Created over the last week\" -r \"iRODS Bot irods@domain>\" recipient@domain Specific queries to monitor catalog state Count of zero Length files where the checksum is not that of an empty file select count(*) from r_data_main d where d.data_size = '0' and to_timestamp(cast(d.create_ts as bigint)) > (NOW() - INTERVAL '7 DAY') and d.data_id not in (select object_id from r_objt_metamap a, r_meta_main b where a.meta_id = b.meta_id and b.meta_attr_name = 'md5' and b.meta_attr_value = 'd41d8cd98f00b204e9800998ecf8427e') and d.resc_id not in (with recursive cte as ( select r.resc_id, cast(r.resc_name as text) as resc_name, cast((case when r.resc_parent = '' then null else r.resc_parent end) as integer), 1 as level from r_resc_main r union all select e.resc_id, c.resc_name || ';' || cast(e.resc_name as text) as resc_name, cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer), c.level + 1 from cte c join r_resc_main e on cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer) = c.resc_id ) select resc_id from cte z where resc_name like 'noReplRoot%') Count of zero Length files created in the last week where the checksum is not that of an empty file select count(*) from r_data_main d where data_size = '0' and d.data_id not in (select object_id from r_objt_metamap a, r_meta_main b where a.meta_id = b.meta_id and b.meta_attr_name = 'md5' and b.meta_attr_value = 'd41d8cd98f00b204e9800998ecf8427e') and d.resc_id not in (with recursive cte as ( select r.resc_id, cast(r.resc_name as text) as resc_name, cast((case when r.resc_parent = '' then null else r.resc_parent end) as integer), 1 as level from r_resc_main r union all select e.resc_id, c.resc_name || ';' || cast(e.resc_name as text) as resc_name, cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer), c.level + 1 from cte c join r_resc_main e on cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer) = c.resc_id ) select resc_id from cte z where resc_name like 'noReplRoot%') List objects with a single replica where they are not in a tree that only has single replicas A bit confusing this one, but impossible to do with GenQuery. The idea behind this query is that there are two trees in a Zone, one called 'root' and one called 'noReplRoot'. As you might surmise, the 'root' tree contains a 'replicate' composite tree hierachy, so there should not be any files with a single replica unless they are in the noReplResc tree. This query lists objects violating those conditions created in the past week; select a.data_id, b.coll_name, a.data_name, to_timestamp(cast(a.create_ts as bigint)) as create_date from r_data_main a, r_coll_main b where a.coll_id = b.coll_id and to_timestamp(cast(a.create_ts as bigint)) > (NOW() - INTERVAL '7 DAY') and a.data_id in (select d.data_id from r_data_main d where resc_id not in ( with recursive cte as ( select r.resc_id, cast(r.resc_name as text) as resc_name, cast((case when r.resc_parent = '' then null else r.resc_parent end) as integer), 1 as level from r_resc_main r union all select e.resc_id, c.resc_name || ';' || cast(e.resc_name as text) as resc_name, cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer), c.level + 1 from cte c join r_resc_main e on cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer) = c.resc_id) select resc_id from cte z where resc_name like 'noReplRoot%') group by d.data_id having count(*) = 1) As above but with no time limit select a.data_id, b.coll_name, a.data_name, to_timestamp(cast(a.create_ts as bigint)) as create_date from r_data_main a, r_coll_main b where a.coll_id = b.coll_id and a.data_id in (select d.data_id from r_data_main d where resc_id not in ( with recursive cte as ( select r.resc_id, cast(r.resc_name as text) as resc_name, cast((case when r.resc_parent = '' then null else r.resc_parent end) as integer), 1 as level from r_resc_main r union all select e.resc_id, c.resc_name || ';' || cast(e.resc_name as text) as resc_name, cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer), c.level + 1 from cte c join r_resc_main e on cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer) = c.resc_id) select resc_id from cte z where resc_name like 'noReplRoot%') group by d.data_id having count(*) = 1) order by a.create_ts desc As above but no time limit and just return a count (for monitoring) select count(*) from ( select d.data_id from r_data_main d where resc_id not in ( with recursive cte as ( select r.resc_id, cast(r.resc_name as text) as resc_name, cast((case when r.resc_parent = '' then null else r.resc_parent end) as integer), 1 as level from r_resc_main r union all select e.resc_id, c.resc_name || ';' || cast(e.resc_name as text) as resc_name, cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer), c.level + 1 from cte c join r_resc_main e on cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer) = c.resc_id ) select resc_id from cte z where resc_name like 'noReplRoot%') group by d.data_id having count(*) = 1) t","title":"Usefull Queries"},{"location":"Queries/#querying-irods","text":"","title":"Querying iRODS"},{"location":"Queries/#adding-specific-queries-from-the-command-line","text":"bash escaping can cause havok when loading in a SQl query, so use this approach to work around that. # create file e.g. # vim lastWeekZeroLengthUnintentional.sql # put that into an environment variable QUERY=$(cat lastWeekZeroLengthUnintentional.sql ) # use the env varaible as a source for the query iadmin asq \"${QUERY}\" lastWeekZeroLengthUnintentional # run the query iquest --no-page --sql lastWeekZeroLengthUnintentional","title":"Adding specific queries from the command line"},{"location":"Queries/#sending-query-output-by-email","text":"iquest --no-page --sql lastWeekFilesSingleReplicaNotnoReplRoot | mail -s \"Single Replica Files Created over the last week\" -r \"iRODS Bot irods@domain>\" recipient@domain","title":"Sending query output by email"},{"location":"Queries/#specific-queries-to-monitor-catalog-state","text":"","title":"Specific queries to monitor catalog state"},{"location":"Queries/#count-of-zero-length-files-where-the-checksum-is-not-that-of-an-empty-file","text":"select count(*) from r_data_main d where d.data_size = '0' and to_timestamp(cast(d.create_ts as bigint)) > (NOW() - INTERVAL '7 DAY') and d.data_id not in (select object_id from r_objt_metamap a, r_meta_main b where a.meta_id = b.meta_id and b.meta_attr_name = 'md5' and b.meta_attr_value = 'd41d8cd98f00b204e9800998ecf8427e') and d.resc_id not in (with recursive cte as ( select r.resc_id, cast(r.resc_name as text) as resc_name, cast((case when r.resc_parent = '' then null else r.resc_parent end) as integer), 1 as level from r_resc_main r union all select e.resc_id, c.resc_name || ';' || cast(e.resc_name as text) as resc_name, cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer), c.level + 1 from cte c join r_resc_main e on cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer) = c.resc_id ) select resc_id from cte z where resc_name like 'noReplRoot%')","title":"Count of zero Length files where the checksum is not that of an empty file"},{"location":"Queries/#count-of-zero-length-files-created-in-the-last-week-where-the-checksum-is-not-that-of-an-empty-file","text":"select count(*) from r_data_main d where data_size = '0' and d.data_id not in (select object_id from r_objt_metamap a, r_meta_main b where a.meta_id = b.meta_id and b.meta_attr_name = 'md5' and b.meta_attr_value = 'd41d8cd98f00b204e9800998ecf8427e') and d.resc_id not in (with recursive cte as ( select r.resc_id, cast(r.resc_name as text) as resc_name, cast((case when r.resc_parent = '' then null else r.resc_parent end) as integer), 1 as level from r_resc_main r union all select e.resc_id, c.resc_name || ';' || cast(e.resc_name as text) as resc_name, cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer), c.level + 1 from cte c join r_resc_main e on cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer) = c.resc_id ) select resc_id from cte z where resc_name like 'noReplRoot%')","title":"Count of zero Length files created in the last week where the checksum is not that of an empty file"},{"location":"Queries/#list-objects-with-a-single-replica-where-they-are-not-in-a-tree-that-only-has-single-replicas","text":"A bit confusing this one, but impossible to do with GenQuery. The idea behind this query is that there are two trees in a Zone, one called 'root' and one called 'noReplRoot'. As you might surmise, the 'root' tree contains a 'replicate' composite tree hierachy, so there should not be any files with a single replica unless they are in the noReplResc tree. This query lists objects violating those conditions created in the past week; select a.data_id, b.coll_name, a.data_name, to_timestamp(cast(a.create_ts as bigint)) as create_date from r_data_main a, r_coll_main b where a.coll_id = b.coll_id and to_timestamp(cast(a.create_ts as bigint)) > (NOW() - INTERVAL '7 DAY') and a.data_id in (select d.data_id from r_data_main d where resc_id not in ( with recursive cte as ( select r.resc_id, cast(r.resc_name as text) as resc_name, cast((case when r.resc_parent = '' then null else r.resc_parent end) as integer), 1 as level from r_resc_main r union all select e.resc_id, c.resc_name || ';' || cast(e.resc_name as text) as resc_name, cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer), c.level + 1 from cte c join r_resc_main e on cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer) = c.resc_id) select resc_id from cte z where resc_name like 'noReplRoot%') group by d.data_id having count(*) = 1)","title":"List objects with a single replica where they are not in a tree that only has single replicas"},{"location":"Queries/#as-above-but-with-no-time-limit","text":"select a.data_id, b.coll_name, a.data_name, to_timestamp(cast(a.create_ts as bigint)) as create_date from r_data_main a, r_coll_main b where a.coll_id = b.coll_id and a.data_id in (select d.data_id from r_data_main d where resc_id not in ( with recursive cte as ( select r.resc_id, cast(r.resc_name as text) as resc_name, cast((case when r.resc_parent = '' then null else r.resc_parent end) as integer), 1 as level from r_resc_main r union all select e.resc_id, c.resc_name || ';' || cast(e.resc_name as text) as resc_name, cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer), c.level + 1 from cte c join r_resc_main e on cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer) = c.resc_id) select resc_id from cte z where resc_name like 'noReplRoot%') group by d.data_id having count(*) = 1) order by a.create_ts desc","title":"As above but with no time limit"},{"location":"Queries/#as-above-but-no-time-limit-and-just-return-a-count-for-monitoring","text":"select count(*) from ( select d.data_id from r_data_main d where resc_id not in ( with recursive cte as ( select r.resc_id, cast(r.resc_name as text) as resc_name, cast((case when r.resc_parent = '' then null else r.resc_parent end) as integer), 1 as level from r_resc_main r union all select e.resc_id, c.resc_name || ';' || cast(e.resc_name as text) as resc_name, cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer), c.level + 1 from cte c join r_resc_main e on cast((case when e.resc_parent = '' then null else e.resc_parent end) as integer) = c.resc_id ) select resc_id from cte z where resc_name like 'noReplRoot%') group by d.data_id having count(*) = 1) t","title":"As above but no time limit and just return a count (for monitoring)"},{"location":"Server_setup/","text":"Server Setup iRODS is very sesntitive to hostmaes, so you must ensure that each server can resolve the names of other servers ) as they believe themselves to be called . This is particularly important when it comes to FQDN's etc, and if dealing with Spilt DNS and similar architectures where servers may resolve differently depending on the client/interface used for the resolution.","title":"Setup"},{"location":"Server_setup/#server-setup","text":"iRODS is very sesntitive to hostmaes, so you must ensure that each server can resolve the names of other servers ) as they believe themselves to be called . This is particularly important when it comes to FQDN's etc, and if dealing with Spilt DNS and similar architectures where servers may resolve differently depending on the client/interface used for the resolution.","title":"Server Setup"},{"location":"Sysadmin/","text":"System Administration Tricks and Tips Configuration Validation Manually editing JSON config files can be fraught with issues. Fortunately, RENCI has built a validator into the system; python /var/lib/irods/scripts/validate_json.py /etc/irods/server_config .json file:///var/lib/irods/configuration_schemas/v3/server_config.json Unfortunately, all THAT told me was that it was an invalid JSON file. Not much help! Next, compare my backup of the config file with the live one. You DO take backups, right?. Of course you do! Enter the JSON-diff tool! Also, not as helpful - that also told me the file wasn't valid JSON. OK, how about a normal linter; jsonlint ? jsonlint-py3 /etc/irods/server_config.json /etc/irods/server_config.json:55:8: Error: Expected a ']' but saw '}' | At line 55, column 8, offset 1938 | Array started at line 54, column 20, offset 1928 /etc/irods/server_config.json:56:6: Error: Object properties (keys) must be string literals, numbers, or identifiers | At line 56, column 6, offset 1947 | Object started at line 1, column 0, offset 0 (AT-START) /etc/irods/server_config.json:56:6: Error: Missing value for object property, expected \":\" | At line 56, column 6, offset 1947 | Object started at line 1, column 0, offset 0 (AT-START) /etc/irods/server_config.json:84:4: Error: Expected a '}' but saw ']' | At line 84, column 4, offset 2867 | Object started at line 1, column 0, offset 0 (AT-START) /etc/irods/server_config.json:84:4: Error: Unexpected text after end of JSON value | At line 84, column 4, offset 2867 /etc/irods/server_config.json: has errors OK, that's helpful, but I'd really like a more succinct 'what did I change'... back to good old diff diff -c /etc/irods/server_config.json /etc/irods/server_config.json.20610.2022-05-04@15:55:10~ *** /etc/irods/server_config.json 2022-05-04 15:55:10.618672763 +0100 --- /etc/irods/server_config.json.20610.2022-05-04@15:55:10~ 2021-09-17 11:13:50.011210000 +0100 *************** *** 52,58 **** \"network\": {}, \"resource\": {}, \"rule_engines\": [ - }, { \"instance_name\": \"irods_rule_engine_plugin-irods_rule_language-instance\", \"plugin_name\": \"irods_rule_engine_plugin-irods_rule_language\", --- 52,57 ---- Acting as another user (Illustrated in #6561 ) A lot of the admin commands have the -M flag for rodsusers to effectively use root like powers to act on an object et al that they do not have an ACL for. However that's not always what you want, sometime you want to act as another user (say to upload a file as them). In this case you can set the environment variable clientUserName , e.g. # as rods account iput a.log /zone1/home/tom # a.log is in 'tom' homedir, but they have not ACL to access it. export clientUserName=tom iput a.log /zone1/home/tom # a.log is now owned by 'tom' Note that this will only work for rodsadmin accounts - normal aka rodsuser users can't do this.","title":"General Notes"},{"location":"Sysadmin/#system-administration-tricks-and-tips","text":"","title":"System Administration Tricks and Tips"},{"location":"Sysadmin/#configuration-validation","text":"Manually editing JSON config files can be fraught with issues. Fortunately, RENCI has built a validator into the system; python /var/lib/irods/scripts/validate_json.py /etc/irods/server_config .json file:///var/lib/irods/configuration_schemas/v3/server_config.json Unfortunately, all THAT told me was that it was an invalid JSON file. Not much help! Next, compare my backup of the config file with the live one. You DO take backups, right?. Of course you do! Enter the JSON-diff tool! Also, not as helpful - that also told me the file wasn't valid JSON. OK, how about a normal linter; jsonlint ? jsonlint-py3 /etc/irods/server_config.json /etc/irods/server_config.json:55:8: Error: Expected a ']' but saw '}' | At line 55, column 8, offset 1938 | Array started at line 54, column 20, offset 1928 /etc/irods/server_config.json:56:6: Error: Object properties (keys) must be string literals, numbers, or identifiers | At line 56, column 6, offset 1947 | Object started at line 1, column 0, offset 0 (AT-START) /etc/irods/server_config.json:56:6: Error: Missing value for object property, expected \":\" | At line 56, column 6, offset 1947 | Object started at line 1, column 0, offset 0 (AT-START) /etc/irods/server_config.json:84:4: Error: Expected a '}' but saw ']' | At line 84, column 4, offset 2867 | Object started at line 1, column 0, offset 0 (AT-START) /etc/irods/server_config.json:84:4: Error: Unexpected text after end of JSON value | At line 84, column 4, offset 2867 /etc/irods/server_config.json: has errors OK, that's helpful, but I'd really like a more succinct 'what did I change'... back to good old diff diff -c /etc/irods/server_config.json /etc/irods/server_config.json.20610.2022-05-04@15:55:10~ *** /etc/irods/server_config.json 2022-05-04 15:55:10.618672763 +0100 --- /etc/irods/server_config.json.20610.2022-05-04@15:55:10~ 2021-09-17 11:13:50.011210000 +0100 *************** *** 52,58 **** \"network\": {}, \"resource\": {}, \"rule_engines\": [ - }, { \"instance_name\": \"irods_rule_engine_plugin-irods_rule_language-instance\", \"plugin_name\": \"irods_rule_engine_plugin-irods_rule_language\", --- 52,57 ----","title":"Configuration Validation"},{"location":"Sysadmin/#acting-as-another-user","text":"(Illustrated in #6561 ) A lot of the admin commands have the -M flag for rodsusers to effectively use root like powers to act on an object et al that they do not have an ACL for. However that's not always what you want, sometime you want to act as another user (say to upload a file as them). In this case you can set the environment variable clientUserName , e.g. # as rods account iput a.log /zone1/home/tom # a.log is in 'tom' homedir, but they have not ACL to access it. export clientUserName=tom iput a.log /zone1/home/tom # a.log is now owned by 'tom' Note that this will only work for rodsadmin accounts - normal aka rodsuser users can't do this.","title":"Acting as another user"}]}